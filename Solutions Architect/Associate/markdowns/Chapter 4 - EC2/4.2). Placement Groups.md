# **EC2 Placement Groups.**

Sometimes we want control over the EC2 Instance placement startegy. **This can be defined by using placement groups.**

When you create a placement group, you specify one of the following strategies for the group:

* **Cluster** - clusters many instances into a low latency group in a single availability zone (on the same rack).
* **Spread** - spreads instances across underlying hardware (max of 7 instances per group per availability zone), should only be used for critical applications.
* **Partition** - spreads instances across many different partitions (which rely on different sets of racks of hardware) within an availability zone. Scales to 100s of EC2 instances per group (think Hadoop, Cassandra, Kafka).

**Rack = Hardware.**

Pro's:
* **Cluster**
    * Great network & low latency between instances.
* **Spread** 
    * Can span across multiple availability zones.
    * Reduced risk of simultaneous failure as instances are on different hardware.
* **Partition**
    * Can span across multiple availability zones in the same region.
    * Can scale up to 100's of EC2 instances at any time.
    * Partitions do not share racks with other partitions, making them isolated from one another when it comes to rack failure.
    * EC2 instances get access to partition information as metadata.

Con's:
* **Cluster** 
    * If the rack fails, then all of the instances fail at the same time.
* **Spread**
    * Limited to 7 instances per availability group per placement group.


Use Cases:
* **Cluster**
    * Big data job that needs to comlete extremely quickly.
    * Application that neews extremely low latency and high network throughput.
* **Spread**
    * Application that needs to maximise high availability across different availability zones.
    * Critical applications where each instance must be isolated from failure from one another.
* **Partition**
    * Kafka.
    * HDFS.
    * Cassandra.